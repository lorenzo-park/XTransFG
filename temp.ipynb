{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0b9c0885cbb37fdc3fd6d0cb10017d704b23f15c020873a8924d4c6df47c5ff0e",
   "display_name": "Python 3.7.10 64-bit ('xfg': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "b9c0885cbb37fdc3fd6d0cb10017d704b23f15c020873a8924d4c6df47c5ff0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from PIL import Image\n",
    "from omegaconf import DictConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from dataset.cub import CUB200\n",
    "from model.vit import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DictConfig({\n",
    "    \"patch_size\": 32,\n",
    "    \"split\": \"non-overlap\",\n",
    "    \"slide_step\": 12,\n",
    "    \"hidden_size\": 768,\n",
    "    \"dropout\": 0.1,\n",
    "    \"max_len\": 100,\n",
    "    \"transformer\": {\n",
    "        \"mlp_dim\": 3072,\n",
    "        \"num_heads\": 12,\n",
    "        \"num_layers\": 12,\n",
    "        \"attention_dropout_rate\": 0.0,\n",
    "    },\n",
    "    \"batch_size\": 16,\n",
    "    \"num_workers\": 8,\n",
    "    \"image_size\": 448,\n",
    "    \"lr\": 3e-2,\n",
    "    \"seed\": 42,\n",
    "    \"momentum\": 0.9,\n",
    "    \"epoch\": 10,\n",
    "    \"gpus\": [0],\n",
    "    \"logger\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitViT(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = VisionTransformer(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.init_dataset()\n",
    "\n",
    "        self.train_accuracy = torchmetrics.Accuracy()\n",
    "        self.val_accuracy = torchmetrics.Accuracy()\n",
    "        self.test_accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        train_acc = self.train_accuracy(torch.argmax(outputs, dim=-1), targets)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"train_acc\", train_acc, on_step=False, on_epoch=True,\n",
    "                sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        self.log(\"train_acc_epoch\", self.train_accuracy.compute(),\n",
    "                prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        \n",
    "        outputs = self.model(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        val_acc = self.val_accuracy(torch.argmax(outputs, dim=-1), targets)\n",
    "\n",
    "        self.log(\"val_acc\", val_acc, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.log(\"val_acc_epoch\", self.val_accuracy.compute(),\n",
    "                prog_bar=True, logger=True, sync_dist=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self.model(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        test_acc = self.test_accuracy(torch.argmax(outputs, dim=-1), targets)\n",
    "\n",
    "        self.log(\"test_acc\", test_acc, on_step=False, on_epoch=True, logger=True,\n",
    "                sync_dist=True)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, logger=True,\n",
    "                sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_epoch_end(self, outs):\n",
    "        test_acc = self.test_accuracy.compute()\n",
    "        self.log(\"test_acc_epoch\", test_acc, logger=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.lr, momentum=self.config.momentum)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.config.batch_size,\n",
    "                        shuffle=True, pin_memory=True, num_workers=self.config.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.config.batch_size,\n",
    "                        pin_memory=True, num_workers=self.config.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.config.batch_size,\n",
    "                        pin_memory=True, num_workers=self.config.num_workers)\n",
    "\n",
    "    def init_dataset(self):\n",
    "        train_transform=transforms.Compose([\n",
    "            transforms.Resize((600, 600), InterpolationMode.BILINEAR),\n",
    "            transforms.RandomCrop((448, 448)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        test_transform=transforms.Compose([\n",
    "            transforms.Resize((600, 600), InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop((448, 448)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.train_set = CUB200(root=\"./data\", train=True, transform=train_transform)\n",
    "        self.test_set = CUB200(root=\"./data\", train=False, transform=test_transform)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params\n",
      "-----------------------------------------------------\n",
      "0 | model          | VisionTransformer | 86.4 M\n",
      "1 | train_accuracy | Accuracy          | 0     \n",
      "2 | val_accuracy   | Accuracy          | 0     \n",
      "3 | test_accuracy  | Accuracy          | 0     \n",
      "-----------------------------------------------------\n",
      "86.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "86.4 M    Total params\n",
      "345.616   Total estimated model params size (MB)\n",
      "Validation sanity check:  50%|█████     | 1/2 [00:01<00:01,  1.63s/it]/root/anaconda3/envs/xfg/lib/python3.7/site-packages/pytorch_lightning/core/step_result.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(value, device=device, dtype=torch.float)\n",
      "                                                                      Global seed set to 42\n",
      "Epoch 0:   1%|          | 6/738 [00:08<18:05,  1.48s/it, loss=6.15, v_num=20, val_acc_epoch=0.000]/root/anaconda3/envs/xfg/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "MisconfigurationException",
     "evalue": "`.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9c80a288b352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLitViT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/xfg/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtested_ckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load_ckpt_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xfg/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__load_ckpt_weights\u001b[0;34m(self, ckpt_path)\u001b[0m\n\u001b[1;32m   1130\u001b[0m                     )\n\u001b[1;32m   1131\u001b[0m                 raise MisconfigurationException(\n\u001b[0;32m-> 1132\u001b[0;31m                     \u001b[0;34mf'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m                 )\n\u001b[1;32m   1134\u001b[0m             \u001b[0;31m# load best weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model."
     ]
    }
   ],
   "source": [
    "if config.logger:\n",
    "    from pytorch_lightning.loggers import WandbLogger\n",
    "    logger = WandbLogger(\n",
    "        project=\"xfg\",\n",
    "        name=f\"vit\"\n",
    "    )\n",
    "else:\n",
    "    logger = pl.loggers.TestTubeLogger(\n",
    "        \"output\", name=f\"vit\")\n",
    "    logger.log_hyperparams(config)\n",
    "\n",
    "pl.seed_everything(config.seed)\n",
    "trainer = pl.Trainer(\n",
    "    precision=16,\n",
    "    deterministic=True,\n",
    "    check_val_every_n_epoch=1,\n",
    "    gpus=config.gpus,\n",
    "    logger=logger,\n",
    "    max_epochs=config.epoch,\n",
    "    weights_summary=\"top\",\n",
    "    # accelerator='ddp',\n",
    ")\n",
    "\n",
    "model = LitViT(config)\n",
    "trainer.fit(model)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}