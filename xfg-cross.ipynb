{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0ef5dddf3396bdd55ac5e0a59d6dc121e77b0322ce4c768edc846bd1ea0f25d09",
   "display_name": "Python 3.7.10 64-bit ('xfg': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "b9c0885cbb37fdc3fd6d0cb10017d704b23f15c020873a8924d4c6df47c5ff0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from PIL import Image\n",
    "from omegaconf import DictConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "\n",
    "from dataset.cub import CUB200\n",
    "from model.xfg import XFG\n",
    "from util import WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DictConfig({\n",
    "    \"patch_size\": 32,\n",
    "    \"split\": \"overlap\",\n",
    "    \"slide_step\": 24,\n",
    "    \"hidden_size\": 768,\n",
    "    \"dropout\": 0.1,\n",
    "    \"max_len\": 100,\n",
    "    \"classifier\": \"token\",\n",
    "    \"transformer\": {\n",
    "        \"mlp_dim\": 3072,\n",
    "        \"num_heads\": 12,\n",
    "        \"num_layers\": 12,\n",
    "        \"num_layers_cross\": 12,\n",
    "        \"attention_dropout_rate\": 0.0,\n",
    "    },\n",
    "    \"num_classes\": 200,\n",
    "    \"batch_size\": 16,\n",
    "    \"num_workers\": 8,\n",
    "    \"image_size\": 448,\n",
    "    \"lr\": 3e-2,\n",
    "    \"seed\": 42,\n",
    "    \"momentum\": 0.9,\n",
    "    \"epoch\": 30,\n",
    "    \"gpus\": [0],\n",
    "    \"logger\": True,\n",
    "    \"pretrained_dir\": \"./pretrained/vit/imagenet21k_ViT-B_32.npz\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform=transforms.Compose([\n",
    "    transforms.Resize((600, 600), InterpolationMode.BILINEAR),\n",
    "    transforms.RandomCrop((448, 448)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "train_set = CUB200(root=\"./data\", train=True, caption=True, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XFG(config)\n",
    "# model.load_from(np.load(config.pretrained_dir))\n",
    "# imgs, txts, targets = train_set[0]\n",
    "# imgs = torch.Tensor(imgs).unsqueeze(0)\n",
    "# txts = torch.Tensor(txts)\n",
    "# model(imgs, txts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "load_pretrained: grid-size from 7 to 18\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlorenzopark\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.31<br/>\n                Syncing run <strong style=\"color:#cdcd00\">vit</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/lorenzopark/xfg\" target=\"_blank\">https://wandb.ai/lorenzopark/xfg</a><br/>\n                Run page: <a href=\"https://wandb.ai/lorenzopark/xfg/runs/i7jio1gd\" target=\"_blank\">https://wandb.ai/lorenzopark/xfg/runs/i7jio1gd</a><br/>\n                Run data is saved locally in <code>/home/lorenzo-lab/project/xfg/wandb/run-20210529_015340-i7jio1gd</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n  | Name           | Type     | Params\n--------------------------------------------\n0 | model          | XFG      | 95.4 M\n1 | train_accuracy | Accuracy | 0     \n2 | val_accuracy   | Accuracy | 0     \n3 | test_accuracy  | Accuracy | 0     \n--------------------------------------------\n95.4 M    Trainable params\n0         Non-trainable params\n95.4 M    Total params\n381.747   Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96d44acdca824d188f5eed82c9f842c1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/lorenzo-lab/anaconda3/envs/xfg/lib/python3.7/site-packages/pytorch_lightning/core/step_result.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(value, device=device, dtype=torch.float)\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dda873cc22b8442fb452bcad3ef05966"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ddb23bad9654d528fb09dcefe5852ae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/lorenzo-lab/anaconda3/envs/xfg/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "MisconfigurationException",
     "evalue": "`.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-af94c9f2bf29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLitXFG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/xfg/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtested_ckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load_ckpt_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xfg/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__load_ckpt_weights\u001b[0;34m(self, ckpt_path)\u001b[0m\n\u001b[1;32m   1130\u001b[0m                     )\n\u001b[1;32m   1131\u001b[0m                 raise MisconfigurationException(\n\u001b[0;32m-> 1132\u001b[0;31m                     \u001b[0;34mf'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m                 )\n\u001b[1;32m   1134\u001b[0m             \u001b[0;31m# load best weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model."
     ]
    }
   ],
   "source": [
    "if config.logger:\n",
    "    from pytorch_lightning.loggers import WandbLogger\n",
    "    logger = WandbLogger(\n",
    "        project=\"xfg\",\n",
    "        name=f\"vit\"\n",
    "    )\n",
    "else:\n",
    "    logger = pl.loggers.TestTubeLogger(\n",
    "        \"output\", name=f\"vit\")\n",
    "    logger.log_hyperparams(config)\n",
    "\n",
    "pl.seed_everything(config.seed)\n",
    "trainer = pl.Trainer(\n",
    "    precision=16,\n",
    "    deterministic=True,\n",
    "    check_val_every_n_epoch=1,\n",
    "    gpus=config.gpus,\n",
    "    logger=logger,\n",
    "    max_epochs=config.epoch,\n",
    "    weights_summary=\"top\",\n",
    "    # accelerator='ddp',\n",
    ")\n",
    "\n",
    "model = LitXFG(config)\n",
    "trainer.fit(model)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}