# @package _global_
model: vit_roberta_cls

transformer:
  attention_dropout_rate: 0.0
save_path: ${project_root}/pretrained/vit

cls_head:
  hidden_size: 768
  hidden_dropout_prob: 0.1
  num_labels: 200
pretrained_path: ${project_root}/pretrained/roberta

encoder:
  dropout: 0.1
  hidden_size: 768
  transformer:
    mlp_dim: 3072
    num_heads: 12
    num_layers: 1
    attention_dropout_rate: 0.0